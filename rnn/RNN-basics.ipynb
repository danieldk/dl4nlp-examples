{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "\n",
    "As we have seen during the lectures, a GRU is a relatively simple RNN unit that avoids the vanishing/exploding gradient problem. I will first use GRUs as an example, since they are easier to set up than LSTMs and generally work well.\n",
    "\n",
    "Making a recurrent network in Tensorflow will start with constructing a unit. This unit will be used to process the input at every timestep. The GRU unit is called `GRUCell` in Tensorflow. In its basic usage, we just give it one parameter --- the size of the hidden representation. Let us construct a GRU unit with a hidden layer size of 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_cell = rnn.GRUCell(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic unit is not yet useful by itself. As discussed in the RNN lecture, we have to unroll it in time for training and prediction. You could see as creating an RNN cell for every timestep, where the parameters between\n",
    "all the cells are shared. There are a couple of Tensorflow functions that unroll an RNN, but for reasons that\n",
    "will become clear later, we will use the `dynamic_rnn` function. In the following example, we are assuming an\n",
    "input of batch size *512*, with *10* timesteps per instance, and a size of *50* per timestep. This could e.g.\n",
    "be a character embedding of size *50*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnn/transpose:0' shape=(512, 10, 100) dtype=float32>,\n",
       " <tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 100) dtype=float32>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(shape=(512, 10, 50), dtype=tf.float32)\n",
    "gru = tf.nn.dynamic_rnn(gru_cell, x, dtype=tf.float32)\n",
    "gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dynamic_rnn` function takes the cell and the input as its arguments. We also have to specify the data type\n",
    "of the output. `dynamic_rnn` returns a 2-tuple. The first value in the tuple contains the hidden states for all timesteps of all instances. The second value contains the final hidden representation. For the task at hand (word tag\n",
    "prediction) we are only interested in the final hidden representation. Hence, we can discard the first element\n",
    "of the tuple.\n",
    "\n",
    "Note that we assumed that the input is always 10 time steps. Since our inputs are words of a varying length, this\n",
    "is not a reasonable assumption to make. In order to handle all normal words, you could set this dimension of the\n",
    "input to some reasonably large number of characters, or just use the length of the longest word in the \n",
    "training/validation set.\n",
    "\n",
    "However, if we set the *n* to a reasonable number, it would be problematic to return the hidden state at timestep\n",
    "*n* for words that are shorter than *n* characters. This is problematic, because in these cases we will be feeding\n",
    "some timesteps with bogus (e.g. all-zero) representations of characters that do not exist. Luckily, dynamic_rnn\n",
    "provides the `sequence_length` keyword argument. Through this argument, we can add a tensor that specifies the length\n",
    "of each sequence as an additional input. The GRU will then always return the hidden representation at the\n",
    "sequence's length as the final hidden representation. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "gru_cell = rnn.GRUCell(100)\n",
    "x = tf.placeholder(shape=(512, 10, 50), dtype=tf.float32)\n",
    "seq_lens = tf.placeholder(shape=(512,), dtype=tf.int32)\n",
    "_, hidden = tf.nn.dynamic_rnn(gru_cell, x, sequence_length=seq_lens, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides enough to set up an RNN for the word classification task. `hidden` is a regular hidden\n",
    "representation that could e.g. be the input of the `softmax` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "\n",
    "To create a bidirectional RNN, you can pretty much use the same procedure. However, now you need to create two\n",
    "cells: one for the forward RNN and one for the backward RNN. The RNN can then be unrolled using the\n",
    "`bidirectional_dynamic_rnn` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 100) dtype=float32>,\n",
       " <tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 100) dtype=float32>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "forward_cell = rnn.GRUCell(100)\n",
    "backward_cell = rnn.GRUCell(100)\n",
    "x = tf.placeholder(shape=(512, 10, 50), dtype=tf.float32)\n",
    "seq_lens = tf.placeholder(shape=(512,), dtype=tf.int32)\n",
    "_, hidden = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, x, sequence_length=seq_lens, dtype=tf.float32)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `dynamic_rnn` function, `birectional_dynamic_rnn` also returns a 2-tuple. But the second tuple element\n",
    "is now a 2-tuple as well. These values represent the final hidden representation of the forward and backwards\n",
    "RNNs. You can feed them into another layer, by e.g. concatenating them using `tf.concat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization using dropout\n",
    "\n",
    "RNN cells can be regularized using dropout. In order to do so, use a `DropoutWrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_cell = rnn.GRUCell(100)\n",
    "regularized_cell = rnn.DropoutWrapper(gru_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use `regularized_cell` while unrolling the RNN. Note that you can specify the dropout rates with keyword arguments, see the [`DropoutWrapper` documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "Unrolling an LSTM works similar to a GRU:\n",
    "    \n",
    "* Use the `BasicLSTMCell` unit.\n",
    "* Note that the final hidden representation has two outputs: `h` is the hidden representation, `c` the memory cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 100) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm_cell = rnn.BasicLSTMCell(100)\n",
    "x = tf.placeholder(shape=(512, 10, 50), dtype=tf.float32)\n",
    "seq_lens = tf.placeholder(shape=(512,), dtype=tf.int32)\n",
    "_, hidden = tf.nn.dynamic_rnn(lstm_cell, x, sequence_length=seq_lens, dtype=tf.float32)\n",
    "hidden.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
